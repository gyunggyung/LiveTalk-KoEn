import os

# [í•µì‹¬ ìˆ˜ì •] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ ë°©ì§€ (OpenMP ì—ëŸ¬ í•´ê²°)
# ë°˜ë“œì‹œ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ importë³´ë‹¤ ë¨¼ì € ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import numpy as np
import pyaudiowpatch as pyaudio
from faster_whisper import WhisperModel
import threading
import queue
import sys
import time
import traceback

# ==========================================
# âš™ï¸ ì„¤ì •ê°’
# ==========================================
MODEL_SIZE = "Systran/faster-distil-whisper-small.en" 
SAMPLE_RATE = 16000
CHUNK_SIZE = int(SAMPLE_RATE * 0.5)  # 0.5ì´ˆ ë‹¨ìœ„ ì²­í¬
VOLUME_THRESHOLD = 0.0001
# ==========================================

audio_queue = queue.Queue()

def get_default_wasapi_device(p):
    """pyaudiowpatchë¥¼ ì‚¬ìš©í•´ ìœˆë„ìš° ë£¨í”„ë°±(ìŠ¤í”¼ì»¤ ì¶œë ¥ ìº¡ì²˜) ì¥ì¹˜ íƒìƒ‰"""
    try:
        wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)
        default_speakers = p.get_device_info_by_index(wasapi_info["defaultOutputDevice"])
        
        if not default_speakers["isLoopbackDevice"]:
            for loopback in p.get_loopback_device_info_generator():
                if default_speakers["name"] in loopback["name"]:
                    print(f"ğŸ¤ Loopback found: {loopback['name']}")
                    return loopback
                    
        print(f"ğŸ¤ Loopback fallback: {default_speakers['name']}")
        return default_speakers
    except Exception as e:
        print(f"âŒ ì¥ì¹˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def load_stt_model():
    print(f"Loading model '{MODEL_SIZE}' on CPU...")
    try:
        # compute_type="int8"ë¡œ CPU ì†ë„ ìµœì í™”
        model = WhisperModel(MODEL_SIZE, device="cpu", compute_type="int8")
        print("âœ… Model loaded successfully!")
        return model
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

def record_audio_loop():
    """ë…¹ìŒ ìŠ¤ë ˆë“œ - PyAudio WASAPI Loopback"""
    p = pyaudio.PyAudio()
    device = get_default_wasapi_device(p)
    if device is None:
        print("âŒ ì‹œìŠ¤í…œ ì˜¤ë””ì˜¤ë¥¼ ìº¡ì²˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        device_channels = device["maxInputChannels"]
        actual_mic_sr = int(device["defaultSampleRate"])
        
        # PyAudio ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
        stream = p.open(format=pyaudio.paFloat32,
                        channels=device_channels,
                        rate=actual_mic_sr,
                        input=True,
                        input_device_index=device["index"],
                        frames_per_buffer=int(actual_mic_sr * 0.5))
                        
        while True:
            # 0.5ì´ˆ ë‹¨ìœ„ë¡œ ìˆ˜ì‹ 
            data = stream.read(int(actual_mic_sr * 0.5), exception_on_overflow=False)
            
            # byteë¥¼ float32 numpy arrayë¡œ ë³€í™˜
            audio_array = np.frombuffer(data, dtype=np.float32)
            
            # ì±„ë„ì´ 2ê°œ ì´ìƒì´ë©´ ì²« ë²ˆì§¸ ì±„ë„(Mono)ë§Œ ì¶”ì¶œí•˜ì—¬ ë…¸ì´ì¦ˆ ë°©ì§€
            if device_channels > 1:
                audio_array = np.reshape(audio_array, (-1, device_channels))
                audio_array = audio_array[:, 0]
                
            audio_queue.put((audio_array, actual_mic_sr))
            
    except Exception as e:
        print(f"âŒ ë…¹ìŒ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    finally:
        p.terminate()

def process_audio_loop(model):
    """ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
    print("ğŸ“ Ready to transcribe... (ì¬ìƒë˜ëŠ” ì†Œë¦¬ê°€ ì—†ìœ¼ë©´ ëŒ€ê¸°í•©ë‹ˆë‹¤)")
    import scipy.signal
    
    accumulated_audio = np.array([], dtype=np.float32)
    silence_counter = 0

    while True:
        audio_data, mic_samplerate = audio_queue.get()
        # soundcard record returns shape: (frames, channels). Extract the first channel properly, DON'T just flatten it into chaos.
        if audio_data.ndim == 2:
            single_channel_data = audio_data[:, 0]
        else:
            single_channel_data = audio_data
            
        chunk_data = single_channel_data.astype(np.float32)

        # 1. ì›ë³¸ í•´ìƒë„(48kHz ë“±)ì—ì„œ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
        if mic_samplerate != SAMPLE_RATE:
            samples_count = int(len(chunk_data) * SAMPLE_RATE / mic_samplerate)
            chunk_16k = scipy.signal.resample(chunk_data, samples_count)
        else:
            chunk_16k = chunk_data

        # 2. ë³¼ë¥¨ ì²´í¬
        vol = np.abs(chunk_16k).mean()
        
        if vol < VOLUME_THRESHOLD:
            silence_counter += 1
            # 2ë²ˆ ì—°ì†(ì•½ 10ì´ˆ) ë¬´ìŒì´ ì§€ì†ë˜ë©´ ê·¸ë™ì•ˆ ìŒ“ì¸ ê±¸ ì²˜ë¦¬
            if silence_counter >= 2 and len(accumulated_audio) > 0:
                pass # ì•„ë˜ ì²˜ë¦¬ ë¸”ë¡ìœ¼ë¡œ ë„˜ì–´ê°
            else:
                continue
        else:
            silence_counter = 0
            
        # 3. ì˜¤ë””ì˜¤ ëˆ„ì  (ë¬¸ì¥ ë‹¨ìœ„ ì¸ì‹ì„ ìœ„í•´)
        if vol >= VOLUME_THRESHOLD:
             accumulated_audio = np.concatenate((accumulated_audio, chunk_16k))
        
        # 4. ë²„í¼ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ (ì˜ˆ: 15ì´ˆ ì´ìƒ) ê°•ì œ ë¶„ì„, 
        #    í˜¹ì€ ë¬´ìŒ ëˆ„ì ìœ¼ë¡œ ì²˜ë¦¬ ì¡°ê±´ ë‹¬ì„± ì‹œ ë¶„ì„
        if len(accumulated_audio) >= SAMPLE_RATE * 15 or (silence_counter >= 2 and len(accumulated_audio) > 0):
            try:
                audio_array = accumulated_audio.copy()
                
                # ë””ë²„ê¹…ìš©: í˜„ì¬ ë“¤ì–´ì˜¨ ì˜¤ë””ì˜¤ë¥¼ wav íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ê¹¨ì ¸ìˆëŠ”ì§€ í™•ì¸
                import scipy.io.wavfile
                scipy.io.wavfile.write("debug_audio.wav", SAMPLE_RATE, audio_array)
                print("ğŸ’¾ Saved debug_audio.wav for inspection.")
                
                # ì •ê·œí™”
                max_val = np.abs(audio_array).max()
                if max_val > 0:
                    audio_array = audio_array / max_val
                    
                segments, info = model.transcribe(audio_array, beam_size=5, language="en", vad_filter=False, condition_on_previous_text=False)
                
                full_text = []
                for segment in segments:
                    text = segment.text.strip()
                    if text and len(text) >= 2:
                        full_text.append(text)
                
                if full_text:
                    print(f"â–¶ {' '.join(full_text)}")
                    
            except Exception as e:
                print(f"ë³€í™˜ ì˜¤ë¥˜: {e}")
                
            # ë¶„ì„ í›„ ë²„í¼ ì •ë¦¬
            accumulated_audio = np.array([], dtype=np.float32)
            silence_counter = 0

if __name__ == "__main__":
    model = load_stt_model()
    
    # ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì„¤ì •í•˜ì—¬ ë©”ì¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ê°™ì´ ì£½ë„ë¡ ì„¤ì •
    recorder_thread = threading.Thread(target=record_audio_loop, daemon=True)
    recorder_thread.start()
    
    try:
        process_audio_loop(model)
    except KeyboardInterrupt:
        print("\nğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ.")