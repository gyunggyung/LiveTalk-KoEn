import os
import threading
import queue
import time
import traceback
import warnings

# í™˜ê²½ ì„¤ì • (OpenMP ì¶©ëŒ ë°©ì§€)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import numpy as np
import pyaudiowpatch as pyaudio
from faster_whisper import WhisperModel
from flask import Flask, jsonify, render_template_string

# Python 3.13ì—ì„œ cgi ëª¨ë“ˆì´ ì‚­ì œë˜ì–´ googletrans í˜¸í™˜ì„± ë¬¸ì œê°€ ë°œìƒí•˜ë¯€ë¡œ ì„ì‹œ Mock ì ìš©
import sys
if 'cgi' not in sys.modules:
    import types
    mock_cgi = types.ModuleType('cgi')
    mock_cgi.parse_header = lambda header: (header, {})
    sys.modules['cgi'] = mock_cgi

from googletrans import Translator

# ==========================================
# âš™ï¸ ì„¤ì •ê°’
# ==========================================
MODEL_SIZE = "Systran/faster-distil-whisper-small.en"
SAMPLE_RATE = 16000
CHUNK_SIZE = int(SAMPLE_RATE * 0.5)  # 0.5ì´ˆ ë‹¨ìœ„ ì²­í¬
VOLUME_THRESHOLD = 0.0001
# ==========================================

audio_queue = queue.Queue()
transcribed_logs = []  # ì™„ë£Œëœ ë²ˆì—­ ë¡œê·¸ (Final)
current_draft = {"en": "", "ko": ""}  # í˜„ì¬ ì‹¤ì‹œê°„ ì‘ì„±ì¤‘ì¸ ë¬¸ì¥ (Draft)

app = Flask(__name__)

# Python 3.13 í˜¸í™˜ íŒ¨ì¹˜
import sys
if 'cgi' not in sys.modules:
    import types
    mock_cgi = types.ModuleType('cgi')
    mock_cgi.parse_header = lambda header: (header, {})
    sys.modules['cgi'] = mock_cgi

from googletrans import Translator
translator = Translator()

# ==========================================
# ğŸ¨ ì›¹ í˜ì´ì§€ ë””ìì¸ (ë²ˆì—­ ë° ì‹¤ì‹œê°„ Draft í¬í•¨)
# ==========================================
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live Transcription & Translation</title>
    <style>
        body { 
            background-color: #121212; 
            color: #e0e0e0; 
            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; 
            padding: 20px; 
            margin: 0;
        }
        .container { max-width: 1000px; margin: 0 auto; }
        #chat-box { 
            background-color: #1e1e1e; 
            border: 1px solid #333; 
            padding: 40px; 
            height: 65vh; 
            overflow-y: auto; 
            border-radius: 12px;
            font-size: 20px; 
            line-height: 1.6;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
            scroll-behavior: smooth;
        }
        .log-entry { margin-bottom: 25px; padding-bottom: 15px; border-bottom: 1px solid #333; }
        .en-text { color: #aaaaaa; font-size: 18px; margin-bottom: 5px; }
        .ko-text { color: #ffffff; font-size: 24px; font-weight: bold; }
        
        /* ì‹¤ì‹œê°„ìœ¼ë¡œ ë§í•˜ê³  ìˆëŠ” ì¤‘ì¸ ë¬¸ì¥ (Draft) íš¨ê³¼ */
        .draft-entry {
            opacity: 0.7;
            border-bottom: 1px dashed #555;
        }
        .draft-entry .ko-text::after {
            content: ' ...';
            animation: blink 1s infinite steps(1);
        }
        @keyframes blink { 50% { opacity: 0; } }
        
        .btn-group { margin-top: 20px; display: flex; gap: 15px; justify-content: center; }
        button { padding: 15px 25px; cursor: pointer; border: none; border-radius: 8px; font-size: 16px; font-weight: bold; color: white; transition: opacity 0.3s; }
        button:hover { opacity: 0.8; }
        .btn-copy { background-color: #3700b3; }
        .btn-clear { background-color: #cf6679; color: #000; }
    </style>
</head>
<body>
    <div class="container">
        <div id="chat-box">Waiting for audio to translate...</div>
        <div class="btn-group">
            <button class="btn-copy" onclick="copyAll()">ğŸ“‹ ì „ì²´ ë³µì‚¬</button>
            <button class="btn-clear" onclick="clearScreen()">ğŸ—‘ï¸ í™”ë©´ ë¹„ìš°ê¸°</button>
        </div>
    </div>
    <script>
        setInterval(fetchLogs, 500); // 0.5ì´ˆë§ˆë‹¤ ë¹ ë¥´ê²Œ ê°±ì‹  (ì‹¤ì‹œê°„ ì²´ê° ê·¹ëŒ€í™”)
        
        let lastDraftEn = '';
        
        function fetchLogs() {
            fetch('/update')
                .then(response => response.json())
                .then(data => {
                    const box = document.getElementById('chat-box');
                    if (data.logs.length === 0) {
                        if (box.innerHTML.includes('<div class="log-entry">')) {
                             box.innerHTML = "Cleaned! Waiting for new audio...";
                        }
                        return;
                    }
                    
                    const newHtml = data.logs.map(log => `
                        <div class="log-entry ${log.is_draft ? 'draft-entry' : ''}">
                            <div class="en-text">ğŸ‡ºğŸ‡¸ ${log.en}</div>
                            <div class="ko-text">ğŸ‡°ğŸ‡· ${log.ko}</div>
                        </div>
                    `).join('');
                    
                    if (box.innerHTML !== newHtml) {
                        const isAtBottom = box.scrollHeight - box.scrollTop <= box.clientHeight + 50;
                        box.innerHTML = newHtml;
                        if (isAtBottom) {
                            box.scrollTop = box.scrollHeight;
                        }
                    }
                });
        }
        function copyAll() {
            // draft ë¬¸êµ¬, ì•„ì´ì½˜ ë¹¼ê³  í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ê¹”ë”í•˜ê²Œ ë³µì‚¬í•˜ë©´ ë” ì¢‹ì§€ë§Œ ì§€ê¸ˆì€ ì „ì²´ ë³µì‚¬ ìœ ì§€
            const text = document.getElementById('chat-box').innerText;
            navigator.clipboard.writeText(text).then(() => { alert("ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!"); });
        }
        function clearScreen() {
            if(confirm("ì •ë§ ëª¨ë“  ë‚´ìš©ì„ ì§€ìš°ì‹œê² ìŠµë‹ˆê¹Œ?")) {
                fetch('/clear').then(() => { document.getElementById('chat-box').innerHTML = "Resetting..."; });
            }
        }
    </script>
</body>
</html>
"""

@app.route('/')
def index():
    return render_template_string(HTML_TEMPLATE)

@app.route('/update')
def update():
    # í™•ì •ëœ ë¡œê·¸ì— ì‹¤ì‹œê°„ ì‘ì„±ì¤‘ì¸ ì´ˆì•ˆ(Draft)ì„ ë”í•´ì„œ ì›¹ìœ¼ë¡œ ì „ì†¡
    logs_to_send = transcribed_logs.copy()
    if current_draft['en']:
        draft_copy = current_draft.copy()
        draft_copy['is_draft'] = True
        logs_to_send.append(draft_copy)
    return jsonify({'logs': logs_to_send})

@app.route('/clear')
def clear_logs():
    global transcribed_logs, current_draft
    transcribed_logs = []
    current_draft = {"en": "", "ko": ""}
    print("ğŸ§¹ í™”ë©´ê³¼ ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return jsonify({'status': 'cleared'})

# ==========================================
# ë°±ì—”ë“œ ë¡œì§
# ==========================================
def get_default_wasapi_device(p):
    """pyaudiowpatch ìœˆë„ìš° ë£¨í”„ë°± ì¥ì¹˜ íƒìƒ‰"""
    try:
        wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)
        default_speakers = p.get_device_info_by_index(wasapi_info["defaultOutputDevice"])
        
        if not default_speakers["isLoopbackDevice"]:
            for loopback in p.get_loopback_device_info_generator():
                if default_speakers["name"] in loopback["name"]:
                    print(f"ğŸ¤ Loopback found: {loopback['name']}")
                    return loopback
                    
        return default_speakers
    except Exception as e:
        print(f"âŒ ì¥ì¹˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def record_audio_loop():
    p = pyaudio.PyAudio()
    device = get_default_wasapi_device(p)
    if device is None: return

    try:
        device_channels = device["maxInputChannels"]
        actual_mic_sr = int(device["defaultSampleRate"])
        
        stream = p.open(format=pyaudio.paFloat32,
                        channels=device_channels,
                        rate=actual_mic_sr,
                        input=True,
                        input_device_index=device["index"],
                        frames_per_buffer=int(actual_mic_sr * 0.5))
                        
        while True:
            data = stream.read(int(actual_mic_sr * 0.5), exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.float32)
            if device_channels > 1:
                audio_array = np.reshape(audio_array, (-1, device_channels))
                audio_array = audio_array[:, 0]
            audio_queue.put((audio_array, actual_mic_sr))
    except Exception as e:
        print(f"ë…¹ìŒ ì˜¤ë¥˜: {e}")
    finally:
        p.terminate()

def process_audio_loop():
    global current_draft, transcribed_logs
    print("Loading Faster-Whisper model...")
    model = WhisperModel(MODEL_SIZE, device="cpu", compute_type="int8")
    print(f"âœ… Web Server Running on http://127.0.0.1:5001")
    
    import scipy.signal
    accumulated_audio = np.array([], dtype=np.float32)
    silence_counter = 0
    last_translated_en = ""
    last_translated_ko = ""
    
    while True:
        audio_data, mic_samplerate = audio_queue.get()
        
        # 1. 16kHz ë‹¤ìš´ìƒ˜í”Œë§
        if mic_samplerate != SAMPLE_RATE:
            samples_count = int(len(audio_data) * SAMPLE_RATE / mic_samplerate)
            chunk_16k = scipy.signal.resample(audio_data, samples_count)
        else:
            chunk_16k = audio_data

        # 2. ë³¼ë¥¨ ì²´í¬ ë° ë¬´ìŒ ì¹´ìš´í„° ì¦ê°€
        vol = np.abs(chunk_16k).mean()
        if vol < VOLUME_THRESHOLD:
            silence_counter += 1
        else:
            silence_counter = 0

        # ëˆ„ì 
        if len(accumulated_audio) > 0 or vol >= VOLUME_THRESHOLD:
             accumulated_audio = np.concatenate((accumulated_audio, chunk_16k))
        
        # ìŒì„±ì´ ì¡´ì¬í•  ë•Œë§Œ ë¶„ì„ ì‹¤í–‰. ë„ˆë¬´ ì§§ì€(0.5ì´ˆ ë¯¸ë§Œ) ê¸¸ì´ëŠ” ì˜¤ì¸ì‹ ë°©ì§€ë¥¼ ìœ„í•´ ìŠ¤í‚µ
        if len(accumulated_audio) >= SAMPLE_RATE * 0.5:
            try:
                audio_array = accumulated_audio.copy()
                max_val = np.abs(audio_array).max()
                if max_val > 0: audio_array = audio_array / max_val
                    
                # Draft ë²ˆì—­ (ì‹¤ì‹œê°„)
                segments, _ = model.transcribe(audio_array, beam_size=2, language="en", vad_filter=False, condition_on_previous_text=False)
                full_text = [seg.text.strip() for seg in segments if len(seg.text.strip()) > 1]
                
                if full_text:
                    en_text = ' '.join(full_text)
                    
                    # ìƒˆë¡­ê²Œ ë‹¨ì–´ê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œë§Œ ë²ˆì—­ API í˜¸ì¶œ (API ì œí•œ/ì§€ì—° ë°©ì§€)
                    if en_text != last_translated_en:
                        try:
                            # ë²ˆì—­ê¸° í’ˆì§ˆ ê°œì„ : ì „ì²´ ë¬¸ë§¥ì„ í†µì§¸ë¡œ ë„£ìŒ
                            ko_trans = translator.translate(en_text, dest='ko').text
                        except Exception as e:
                            ko_trans = f"[ë²ˆì—­ ì¤‘...]"
                        
                        last_translated_en = en_text
                        last_translated_ko = ko_trans
                        
                    current_draft = {"en": en_text, "ko": last_translated_ko}
            except Exception as e:
                pass

        # 1ì´ˆ ì´ìƒ ë¬´ìŒ(silence_counter >= 2)ì´ê±°ë‚˜ ë²„í¼ê°€ ë„ˆë¬´ ê¸¸ì–´ì§„ ê²½ìš° (ìµœëŒ€ 12ì´ˆ), ë¬¸ì¥ì„ ë§ˆê°(Commit)
        if (silence_counter >= 2 and len(accumulated_audio) > 0) or len(accumulated_audio) > SAMPLE_RATE * 12:
            if current_draft['en']:
                transcribed_logs.append({"en": current_draft['en'], "ko": current_draft['ko'], "is_draft": False})
                print(f"âœ… [ì €ì¥ë¨] {current_draft['en']} -> {current_draft['ko']}")
            
            # ë²„í¼ ë° ì´ˆê¸°í™”
            accumulated_audio = np.array([], dtype=np.float32)
            current_draft = {"en": "", "ko": ""}
            last_translated_en = ""
            last_translated_ko = ""
            silence_counter = 0

if __name__ == "__main__":
    t1 = threading.Thread(target=record_audio_loop, daemon=True)
    t1.start()
    
    t2 = threading.Thread(target=process_audio_loop, daemon=True)
    t2.start()
    
    # ìƒˆ í¬íŠ¸ëŠ” 5001 ì‚¬ìš© (ê¸°ì¡´ web.py ì¶©ëŒ ë°©ì§€)
    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)
