import os
import threading
import queue
import sys
import time
import traceback
import warnings

# í™˜ê²½ ì„¤ì • (OpenMP ì¶©ëŒ ë°©ì§€)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import numpy as np
import pyaudiowpatch as pyaudio
from faster_whisper import WhisperModel
from flask import Flask, jsonify, render_template_string

# ==========================================
# âš™ï¸ ì„¤ì •ê°’
# ==========================================
MODEL_SIZE = "Systran/faster-distil-whisper-small.en"
SAMPLE_RATE = 16000
CHUNK_SIZE = int(SAMPLE_RATE * 0.5)  # 0.5ì´ˆ ë‹¨ìœ„ ì²­í¬
VOLUME_THRESHOLD = 0.0001
# ==========================================

audio_queue = queue.Queue()
transcribed_logs = []  # ì „ì—­ ë¦¬ìŠ¤íŠ¸ (í…ìŠ¤íŠ¸ ì €ì¥ì†Œ)
app = Flask(__name__)

# ==========================================
# ğŸ¨ ì›¹ í˜ì´ì§€ ë””ìì¸ (ìƒëµ)
# ==========================================
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real-time Transcription</title>
    <style>
        body { 
            background-color: #121212; 
            color: #e0e0e0; 
            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; 
            padding: 20px; 
            margin: 0;
        }
        .container { max-width: 900px; margin: 0 auto; }
        #chat-box { 
            background-color: #1e1e1e; 
            border: 1px solid #333; 
            padding: 40px; 
            height: 60vh; 
            overflow-y: auto; 
            border-radius: 12px;
            font-size: 20px; 
            line-height: 1.8;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        .log-entry { margin-bottom: 20px; color: #cccccc; }
        .btn-group { margin-top: 20px; display: flex; gap: 15px; justify-content: center; }
        button { padding: 15px 25px; cursor: pointer; border: none; border-radius: 8px; font-size: 16px; font-weight: bold; color: white; transition: opacity 0.3s; }
        button:hover { opacity: 0.8; }
        .btn-copy { background-color: #3700b3; }
        .btn-trans { background-color: #018786; }
        .btn-clear { background-color: #cf6679; color: #000; }
    </style>
</head>
<body>
    <div class="container">
        <div id="chat-box">Waiting for audio...</div>
        <div class="btn-group">
            <button class="btn-copy" onclick="copyAll()">ğŸ“‹ ì „ì²´ ë³µì‚¬</button>
            <button class="btn-trans" onclick="window.open('https://translate.google.com', '_blank')">ğŸŒ êµ¬ê¸€ ë²ˆì—­ê¸°</button>
            <button class="btn-clear" onclick="clearScreen()">ğŸ—‘ï¸ í™”ë©´ ë¹„ìš°ê¸°</button>
        </div>
    </div>
    <script>
        setInterval(fetchLogs, 1000);
        function fetchLogs() {
            fetch('/update')
                .then(response => response.json())
                .then(data => {
                    const box = document.getElementById('chat-box');
                    if (data.logs.length === 0) {
                        if (box.innerHTML.includes('<div class="log-entry">')) {
                             box.innerHTML = "Cleaned! Waiting for new audio...";
                        }
                        return;
                    }
                    const newHtml = data.logs.map(log => `<div class="log-entry">${log.text}</div>`).join('');
                    if (box.innerHTML !== newHtml) {
                        box.innerHTML = newHtml;
                        box.scrollTop = box.scrollHeight;
                    }
                });
        }
        function copyAll() {
            const text = document.getElementById('chat-box').innerText;
            navigator.clipboard.writeText(text).then(() => { alert("ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!"); });
        }
        function clearScreen() {
            if(confirm("ì •ë§ ëª¨ë“  ë‚´ìš©ì„ ì§€ìš°ì‹œê² ìŠµë‹ˆê¹Œ?")) {
                fetch('/clear').then(() => { document.getElementById('chat-box').innerHTML = "Resetting..."; });
            }
        }
    </script>
</body>
</html>
"""

@app.route('/')
def index():
    return render_template_string(HTML_TEMPLATE)

@app.route('/update')
def update():
    return jsonify({'logs': transcribed_logs})

@app.route('/clear')
def clear_logs():
    global transcribed_logs
    transcribed_logs = []
    print("ğŸ§¹ í™”ë©´ê³¼ ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return jsonify({'status': 'cleared'})

# ==========================================
# ë°±ì—”ë“œ ë¡œì§
# ==========================================
def get_default_wasapi_device(p):
    """pyaudiowpatchë¥¼ ì‚¬ìš©í•´ ìœˆë„ìš° ë£¨í”„ë°±(ìŠ¤í”¼ì»¤ ì¶œë ¥ ìº¡ì²˜) ì¥ì¹˜ ê°•ì œ íƒìƒ‰"""
    try:
        wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)
        default_speakers = p.get_device_info_by_index(wasapi_info["defaultOutputDevice"])
        
        if not default_speakers["isLoopbackDevice"]:
            for loopback in p.get_loopback_device_info_generator():
                if default_speakers["name"] in loopback["name"]:
                    print(f"ğŸ¤ Loopback found: {loopback['name']}")
                    return loopback
                    
        print(f"ğŸ¤ Loopback fallback: {default_speakers['name']}")
        return default_speakers
    except Exception as e:
        print(f"âŒ ì¥ì¹˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def record_audio_loop():
    p = pyaudio.PyAudio()
    device = get_default_wasapi_device(p)
    if device is None:
        print("âŒ ì‹œìŠ¤í…œ ì˜¤ë””ì˜¤ë¥¼ ìº¡ì²˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        device_channels = device["maxInputChannels"]
        actual_mic_sr = int(device["defaultSampleRate"])
        
        # PyAudio ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
        stream = p.open(format=pyaudio.paFloat32,
                        channels=device_channels,
                        rate=actual_mic_sr,
                        input=True,
                        input_device_index=device["index"],
                        frames_per_buffer=int(actual_mic_sr * 0.5))
                        
        while True:
            # 0.5ì´ˆ ë‹¨ìœ„ë¡œ ìˆ˜ì‹ 
            data = stream.read(int(actual_mic_sr * 0.5), exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.float32)
            
            # ìœˆë„ìš° ì±„ë„ ë¶„ë¦¬ (Mono ì¶”ì¶œ)
            if device_channels > 1:
                audio_array = np.reshape(audio_array, (-1, device_channels))
                audio_array = audio_array[:, 0]
                
            audio_queue.put((audio_array, actual_mic_sr))
            
    except Exception as e:
        print(f"ë…¹ìŒ ì˜¤ë¥˜: {e}")
    finally:
        p.terminate()

def process_audio_loop():
    print("Loading model...")
    model = WhisperModel(MODEL_SIZE, device="cpu", compute_type="int8")
    print(f"âœ… Web Server Running on http://127.0.0.1:5000")
    
    import scipy.signal
    accumulated_audio = np.array([], dtype=np.float32)
    silence_counter = 0
    
    while True:
        audio_data, mic_samplerate = audio_queue.get()
        
        # 1. ì›ë³¸ í•´ìƒë„(48kHz ë“±)ì—ì„œ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
        if mic_samplerate != SAMPLE_RATE:
            samples_count = int(len(audio_data) * SAMPLE_RATE / mic_samplerate)
            chunk_16k = scipy.signal.resample(audio_data, samples_count)
        else:
            chunk_16k = audio_data

        # 2. ë³¼ë¥¨ ì²´í¬
        vol = np.abs(chunk_16k).mean()
        
        if vol < VOLUME_THRESHOLD:
            silence_counter += 1
            if silence_counter >= 2 and len(accumulated_audio) > 0:
                pass 
            else:
                continue
        else:
            silence_counter = 0

        # 3. ì˜¤ë””ì˜¤ ëˆ„ì 
        if vol >= VOLUME_THRESHOLD:
             accumulated_audio = np.concatenate((accumulated_audio, chunk_16k))
        
        # 4. ë¶„ì„ ì§„í–‰ (ì‹¤ì‹œê°„ì„±ì„ ìœ„í•´ 3ì´ˆ ë‹¨ìœ„ í˜¹ì€ ë¬´ìŒ 1ì´ˆ(ì¹´ìš´í„° 2) ë„ë‹¬ ì‹œ ë°”ë¡œ ë²ˆì—­)
        if len(accumulated_audio) >= SAMPLE_RATE * 3 or (silence_counter >= 2 and len(accumulated_audio) > 0):
            try:
                audio_array = accumulated_audio.copy()
                max_val = np.abs(audio_array).max()
                if max_val > 0:
                    audio_array = audio_array / max_val
                    
                # ì‹ ì†í•œ ì²˜ë¦¬ë¥¼ ìœ„í•´ beam_sizeë¥¼ 1ë¡œ ë‚®ì¶°ë„ ë©ë‹ˆë‹¤ (ì •í™•ë„ vs ì†ë„ ì¡°ì ˆ)
                segments, info = model.transcribe(audio_array, beam_size=2, language="en", vad_filter=False, condition_on_previous_text=False)
                
                full_text = []
                for segment in segments:
                    text = segment.text.strip()
                    if text and len(text) > 1:  # ì§§ì€ ê°íƒ„ì‚¬ë„ ì¡ë„ë¡ ì¡°ê±´ ì™„í™”
                        full_text.append(text)
                
                if full_text:
                    final_text = ' '.join(full_text)
                    print(f"ğŸ’¬ {final_text}")
                    transcribed_logs.append({"text": final_text})
                    
            except Exception as e:
                print(f"ë³€í™˜ ì˜¤ë¥˜: {e}")
                
            accumulated_audio = np.array([], dtype=np.float32)
            silence_counter = 0

if __name__ == "__main__":
    t1 = threading.Thread(target=record_audio_loop, daemon=True)
    t1.start()
    
    t2 = threading.Thread(target=process_audio_loop, daemon=True)
    t2.start()
    
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)